# Image Classification with Multi-Layer Perceptron on MNIST Dataset

# Project Overview
In this project, we build a Multi-Layer Perceptron (MLP) to classify handwritten digits from the MNIST dataset. MLPs are a class of feedforward artificial neural networks that consist of multiple layers of neurons, where each layer is fully connected to the next one. This architecture enables the model to learn complex patterns and relationships within the data.

# What is a Multi-Layer Perceptron (MLP)?
An MLP is composed of an input layer, one or more hidden layers, and an output layer. The neurons in the hidden layers apply activation functions to their inputs, allowing the model to capture nonlinear relationships. MLPs are particularly well-suited for classification tasks, such as image recognition, where they can learn to identify patterns in pixel data.

# Dataset
We utilize the MNIST dataset, which contains 70,000 images of handwritten digits (0-9). Each image is 28x28 pixels, and the dataset is commonly used as a benchmark in the machine learning community.

# Conclusion
In this project, we successfully implemented a Multi-Layer Perceptron for classifying handwritten digits from the MNIST dataset. Through this exercise, we learned about the architecture of MLPs, the importance of data preprocessing, and how to evaluate and visualize model performance.

Feel free to explore and modify the code to enhance model performance, such as by adjusting the number of layers, neurons, or training epochs.
